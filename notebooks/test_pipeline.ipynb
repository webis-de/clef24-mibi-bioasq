{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from mibi.modules import DocumentsModule, Question\n",
    "from test_utils import (\n",
    "    get_snippets,\n",
    "    PubMedApiRetrieve,\n",
    "    rerank_biencoder,\n",
    "    rerank_crossencoder,\n",
    "    get_snippets,\n",
    "    get_offset,\n",
    "    response_exact_answer,\n",
    "    response_ideal_answer,\n",
    "    flat_list,\n",
    "    get_snippets_blablador,\n",
    "    retrieve_bm25,\n",
    "    remove_stopwords_and_punctuation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi Alexander,\n",
    "# thanks for contacting us. Indeed, there was a mistake in the filename, not the contents of the file, which is now fixed.\n",
    "# New file (name) BioASQ-task12bPhaseA-testset1\n",
    "\n",
    "TEST_FILE = \"temp/test_set/BioASQ-task11bPhaseA-testset1\"\n",
    "\n",
    "with open(TEST_FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[\"questions\"]\n",
    "print(data[0].keys())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVE_TOP_K = 200\n",
    "RERANK_BM25 = 50\n",
    "RERANK_CROSS = 25\n",
    "RERANK_BI = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(\n",
    "        id=q[\"id\"],\n",
    "        type=q[\"type\"],\n",
    "        body=q[\"body\"],\n",
    "    )\n",
    "    for q in data\n",
    "]\n",
    "retrieve_abstracts = PubMedApiRetrieve(verbose=True, num_results=RETRIEVE_TOP_K)\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    retrieved = retrieve_abstracts.transform([question])\n",
    "    if retrieved.empty:\n",
    "        question_new = copy.deepcopy(question)\n",
    "        question_new.body = remove_stopwords_and_punctuation(question_new.body)\n",
    "        print(f\"trying new query {question_new.body}\")\n",
    "        print(f\"original question: {question.body}\")\n",
    "        etrieved = retrieve_abstracts.transform([question_new])\n",
    "\n",
    "    while retrieved.empty:\n",
    "        question_new.body = \" \".join(question_new.body.split()[:-1])\n",
    "        print(f\"trying new query {question_new.body}\")\n",
    "        print(f\"original question: {question.body}\")\n",
    "        retrieved = retrieve_abstracts.transform([question_new])\n",
    "\n",
    "    retrieved[\"bm25\"] = retrieve_bm25(question, retrieved)\n",
    "    reranked = retrieved.sort_values(\"bm25\", ascending=False).head(RERANK_BM25)\n",
    "    # reranked.to_csv(f\"temp/reranked/bm25_{question.id}.csv\", index=False)\n",
    "\n",
    "    reranked[\"cos_sim\"] = rerank_crossencoder(question, reranked)\n",
    "    reranked = reranked.sort_values(\"cos_sim\", ascending=False).head(RERANK_CROSS)\n",
    "    reranked = reranked.drop(\"cos_sim\", axis=1)\n",
    "\n",
    "    reranked[\"cos_sim\"] = rerank_biencoder(question, reranked)\n",
    "    reranked = reranked.sort_values(\"cos_sim\", ascending=False).head(RERANK_BI)\n",
    "\n",
    "    reranked[\"question\"] = [question.body] * len(reranked)\n",
    "    reranked[\"questionno\"] = [question.id] * len(reranked)\n",
    "    reranked[\"questiontype\"] = [question.type] * len(reranked)\n",
    "\n",
    "    reranked.to_csv(f\"temp/reranked/{question.id}.csv\", index=False)\n",
    "    # retrieved.to_csv(f\"temp/reranked/retrieved_{question.id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API for snippets (SKIP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "IN_DIR = \"temp/reranked/\"\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR)):\n",
    "    reranked = pd.read_csv(os.path.join(IN_DIR, file))\n",
    "    reranked[\"snippets\"] = reranked.apply(\n",
    "        lambda x: get_snippets(x[\"question\"], x[\"title\"], x[\"text\"]), axis=1\n",
    "    )\n",
    "    reranked[\"title_snippets\"] = [x[0] for x in reranked[\"snippets\"]]\n",
    "    reranked[\"abstract_snippets\"] = [x[1] for x in reranked[\"snippets\"]]\n",
    "    reranked[\"offset_title\"] = reranked.apply(\n",
    "        lambda x: get_offset(x[\"title_snippets\"], x[\"text\"]), axis=1\n",
    "    )\n",
    "    reranked[\"offset_abstract\"] = reranked.apply(\n",
    "        lambda x: get_offset(x[\"abstract_snippets\"], x[\"text\"]), axis=1\n",
    "    )\n",
    "    reranked.to_csv(f\"temp/snippets/openai/{file}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API for answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = \"pyterrier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTerrier snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = f\"temp/batch_2/snippets/{choice}/\"\n",
    "# OUT_DIR = f\"temp/batch_2/answers/openai/{choice}/\"\n",
    "\n",
    "processed_files = os.listdir(\"temp/batch_2/answers/openai/pyterrier_snippets/\")\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR)):\n",
    "    if file not in processed_files:\n",
    "        reranked = pd.read_csv(os.path.join(IN_DIR, file))\n",
    "\n",
    "        reranked[\"answer_snippets_exact\"] = [\n",
    "            response_exact_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \" \".join(reranked[\"text\"].tolist()),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked[\"answer_snippets_ideal\"] = [\n",
    "            response_ideal_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \" \".join(reranked[\"text\"].tolist()),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked.to_csv(\n",
    "            f\"temp/batch_2/answers/openai/pyterrier_snippets/{file}\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTerrier abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = f\"temp/batch_2/reranked/{choice}/\"\n",
    "processed_files = os.listdir(\"temp/batch_2/answers/openai/pyterrier_docs/\")\n",
    "\n",
    "ABSTRACTS_FOR_ANSWER = 3\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR)):\n",
    "    if file not in processed_files:\n",
    "        reranked = pd.read_csv(os.path.join(IN_DIR, file))\n",
    "\n",
    "        reranked[\"answer_abstracts_exact\"] = [\n",
    "            response_exact_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \" \".join(\n",
    "                    [a for a in reranked[\"abstract\"].tolist() if str(a) != \"nan\"][\n",
    "                        :ABSTRACTS_FOR_ANSWER\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked[\"answer_abstracts_ideal\"] = [\n",
    "            response_ideal_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \" \".join(\n",
    "                    [a for a in reranked[\"abstract\"].tolist() if str(a) != \"nan\"][\n",
    "                        :ABSTRACTS_FOR_ANSWER\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked.to_csv(\n",
    "            f\"temp/batch_2/answers/openai/pyterrier_docs/{file}\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS_FOR_ANSWER = 3\n",
    "\n",
    "COLUMNS_FOR_ANSWER = {\n",
    "    \"openai\": [\"abstract_snippets\", \"title_snippets\"],\n",
    "    \"gpt\": [\"abstract_snippets_gpt\", \"title_snippets_gpt\"],\n",
    "    \"mistral\": [\n",
    "        [\"abstract_snippets_mistral\", \"title_snippets_mistral\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR)):\n",
    "    if file not in processed_files:\n",
    "        reranked = pd.read_csv(os.path.join(IN_DIR, file))\n",
    "\n",
    "        reranked[\"answer_abstracts_exact\"] = [\n",
    "            response_exact_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \" \".join(\n",
    "                    [a for a in reranked[\"text\"].tolist() if str(a) != \"nan\"][\n",
    "                        :ABSTRACTS_FOR_ANSWER\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked[\"answer_abstracts_ideal\"] = [\n",
    "            response_ideal_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \" \".join(\n",
    "                    [a for a in reranked[\"text\"].tolist() if str(a) != \"nan\"][\n",
    "                        :ABSTRACTS_FOR_ANSWER\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked[\"answer_snippets_exact\"] = [\n",
    "            response_exact_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \"\".join(\n",
    "                    flat_list(reranked[COLUMNS_FOR_ANSWER[choice][0][0]].tolist())\n",
    "                    + flat_list(reranked[COLUMNS_FOR_ANSWER[choice][0][1]].tolist())\n",
    "                ),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked[\"answer_snippets_ideal\"] = [\n",
    "            response_ideal_answer(\n",
    "                reranked[\"question\"].tolist()[0],\n",
    "                reranked[\"questiontype\"].tolist()[0],\n",
    "                \"\".join(\n",
    "                    flat_list(reranked[COLUMNS_FOR_ANSWER[choice][0][0]].tolist())\n",
    "                    + flat_list(reranked[COLUMNS_FOR_ANSWER[choice][0][1]].tolist())\n",
    "                ),\n",
    "            )\n",
    "        ] * len(reranked)\n",
    "        reranked.to_csv(f\"temp/answers/openai/{choice}/{file}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for PyTerrier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answers based on snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_exact_answer(row, ABSTRACTS_OR_SNIPPETS):\n",
    "    if row.questiontype == \"yesno\":\n",
    "        return row[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"][0]\n",
    "    elif row.questiontype == \"summary\":\n",
    "        return row[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"][0]\n",
    "    else:\n",
    "        return row[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"]\n",
    "\n",
    "\n",
    "def return_snippets(reranked):\n",
    "    output = []\n",
    "    for _, row in reranked.iterrows():\n",
    "        d = {\n",
    "            \"document\": row[\"url\"],\n",
    "            \"text\": row[\"text\"],\n",
    "            \"offsetInBeginSection\": row[\"snippet_offset_in_begin_section\"],\n",
    "            \"offsetInEndSection\": row[\"snippet_offset_in_end_section\"],\n",
    "            \"beginSection\": row[\"snippet_begin_section\"],\n",
    "            \"endSection\": row[\"snippet_end_section\"],\n",
    "        }\n",
    "        output.append(d)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def fix_list(l):\n",
    "    l_out = []\n",
    "    for item in l:\n",
    "        if type(item) == list:\n",
    "            if len(item) == 1 and [item] not in l_out:\n",
    "                l_out.append(item)\n",
    "            elif len(item) > 1:\n",
    "                for subitem in item:\n",
    "                    if [subitem] not in l_out:\n",
    "                        l_out.append([subitem])\n",
    "        if type(item) == str and [item] not in l_out:\n",
    "            l_out.append([item])\n",
    "\n",
    "    return l_out[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR1 = \"temp/batch_2/answers/openai/pyterrier_snippets/\"\n",
    "IN_DIR2 = \"temp/batch_2/reranked/pyterrier/\"\n",
    "\n",
    "ABSTRACTS_OR_SNIPPETS = \"snippets\"\n",
    "\n",
    "json_list = []\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR1)):\n",
    "    reranked1 = pd.read_csv(os.path.join(IN_DIR1, file))\n",
    "    # reranked2 = pd.read_csv(os.path.join(IN_DIR2, file))\n",
    "    reranked1[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"] = reranked1[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked1[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"] = reranked1[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"\n",
    "    ].apply(ast.literal_eval)\n",
    "\n",
    "    if reranked1[\"questiontype\"].tolist()[0] in (\"factoid\", \"list\"):\n",
    "        exact_answer = fix_list(\n",
    "            return_exact_answer(reranked1.iloc[0], ABSTRACTS_OR_SNIPPETS)\n",
    "        )\n",
    "    else:\n",
    "        exact_answer = return_exact_answer(reranked1.iloc[0], ABSTRACTS_OR_SNIPPETS)\n",
    "    json_list.append(\n",
    "        {\n",
    "            \"type\": reranked1[\"questiontype\"].tolist()[0],\n",
    "            \"body\": reranked1[\"question\"].tolist()[0],\n",
    "            \"id\": reranked1[\"questionno\"].tolist()[0],\n",
    "            \"ideal_answer\": reranked1[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"].tolist()[\n",
    "                0\n",
    "            ][0],\n",
    "            \"exact_answer\": exact_answer,\n",
    "            # \"documents\": reranked2[\"url\"].tolist(),\n",
    "            # \"snippets\": return_snippets(reranked1),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    json_out = {\"questions\": json_list}\n",
    "\n",
    "with open(\"temp/batch_2/submission/mibi_rag_snippet.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers based on abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR1 = \"temp/batch_2/answers/openai/pyterrier_docs/\"\n",
    "\n",
    "ABSTRACTS_OR_SNIPPETS = \"abstracts\"\n",
    "\n",
    "json_list = []\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR1)):\n",
    "    reranked1 = pd.read_csv(os.path.join(IN_DIR1, file))\n",
    "    # reranked2 = pd.read_csv(os.path.join(IN_DIR2, file))\n",
    "    reranked1[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"] = reranked1[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked1[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"] = reranked1[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"\n",
    "    ].apply(ast.literal_eval)\n",
    "\n",
    "    if reranked1[\"questiontype\"].tolist()[0] in (\"factoid\", \"list\"):\n",
    "        exact_answer = fix_list(\n",
    "            return_exact_answer(reranked1.iloc[0], ABSTRACTS_OR_SNIPPETS)\n",
    "        )\n",
    "    else:\n",
    "        exact_answer = return_exact_answer(reranked1.iloc[0], ABSTRACTS_OR_SNIPPETS)\n",
    "    json_list.append(\n",
    "        {\n",
    "            \"type\": reranked1[\"questiontype\"].tolist()[0],\n",
    "            \"body\": reranked1[\"question\"].tolist()[0],\n",
    "            \"id\": reranked1[\"questionno\"].tolist()[0],\n",
    "            \"ideal_answer\": reranked1[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"].tolist()[\n",
    "                0\n",
    "            ][0],\n",
    "            \"exact_answer\": exact_answer,\n",
    "            # \"documents\": reranked1[\"url\"].tolist(),\n",
    "            # \"snippets\": return_snippets(reranked1),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    json_out = {\"questions\": json_list}\n",
    "\n",
    "with open(\"temp/batch_2/submission/mibi_rag_abstract.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_exact_answer(row, ABSTRACTS_OR_SNIPPETS):\n",
    "    if row.questiontype == \"yesno\":\n",
    "        return row[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"][0]\n",
    "    elif row.questiontype == \"summary\":\n",
    "        return row[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"][0]\n",
    "    else:\n",
    "        return row[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"]\n",
    "\n",
    "\n",
    "def return_snippets(reranked, choice):\n",
    "    output = []\n",
    "    for _, row in reranked.iterrows():\n",
    "        is_success = False\n",
    "        if row[f\"abstract_snippets_{choice}\"]:\n",
    "            for i, snippet in enumerate(row[f\"abstract_snippets_{choice}\"]):\n",
    "                try:\n",
    "                    d = {\n",
    "                        \"document\": row[\"url\"],\n",
    "                        \"text\": snippet,\n",
    "                        \"offsetInBeginSection\": row[f\"offset_abstract_{choice}\"][i][0],\n",
    "                        \"offsetInEndSection\": row[f\"offset_abstract_{choice}\"][i][1],\n",
    "                        \"beginSection\": \"abstract\",\n",
    "                        \"endSection\": \"abstract\",\n",
    "                    }\n",
    "                    output.append(d)\n",
    "                    is_success = True\n",
    "                except:\n",
    "                    pass\n",
    "        if not is_success:\n",
    "            if row[f\"title_snippets_{choice}\"]:\n",
    "                for i, snippet in enumerate(row[f\"title_snippets_{choice}\"]):\n",
    "                    try:\n",
    "                        d = {\n",
    "                            \"document\": row[\"url\"],\n",
    "                            \"text\": snippet,\n",
    "                            \"offsetInBeginSection\": row[f\"offset_title_{choice}\"][i][0],\n",
    "                            \"offsetInEndSection\": row[f\"offset_title_{choice}\"][i][1],\n",
    "                            \"beginSection\": \"title\",\n",
    "                            \"endSection\": \"title\",\n",
    "                        }\n",
    "                        output.append(d)\n",
    "                    except:\n",
    "                        pass\n",
    "    return output\n",
    "\n",
    "\n",
    "def fix_list(l):\n",
    "    l_out = []\n",
    "    for item in l:\n",
    "        if type(item) == list:\n",
    "            if len(item) == 1 and [item] not in l_out:\n",
    "                l_out.append(item)\n",
    "            elif len(item) > 1:\n",
    "                for subitem in item:\n",
    "                    if [subitem] not in l_out:\n",
    "                        l_out.append([subitem])\n",
    "        if type(item) == str and [item] not in l_out:\n",
    "            l_out.append([item])\n",
    "\n",
    "    return l_out[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers based on snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR1 = f\"temp/answers/openai/{choice}\"\n",
    "IN_DIR2 = \"temp/snippets/gpt\"\n",
    "IN_DIR3 = \"temp/snippets/mistral\"\n",
    "\n",
    "ABSTRACTS_OR_SNIPPETS = \"snippets\"\n",
    "\n",
    "json_list = []\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR1)):\n",
    "    reranked1 = pd.read_csv(os.path.join(IN_DIR1, file))\n",
    "    # reranked2 = pd.read_csv(os.path.join(IN_DIR2, file))\n",
    "    reranked3 = pd.read_csv(os.path.join(IN_DIR3, file))\n",
    "    # reranked_merged = pd.concat([reranked1, reranked2, reranked3], axis=1)\n",
    "    reranked_merged = pd.concat([reranked1, reranked3], axis=1)\n",
    "    reranked_merged = reranked_merged.loc[:, ~reranked_merged.columns.duplicated()]\n",
    "    reranked_merged[f\"title_snippets_{choice}\"] = reranked_merged[\n",
    "        f\"title_snippets_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"abstract_snippets_{choice}\"] = reranked_merged[\n",
    "        f\"abstract_snippets_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"offset_title_{choice}\"] = reranked_merged[\n",
    "        f\"offset_title_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"offset_abstract_{choice}\"] = reranked_merged[\n",
    "        f\"offset_abstract_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"] = reranked_merged[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"] = reranked_merged[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"\n",
    "    ].apply(ast.literal_eval)\n",
    "\n",
    "    if reranked_merged[\"questiontype\"].tolist()[0] in (\"factoid\", \"list\"):\n",
    "        exact_answer = fix_list(\n",
    "            return_exact_answer(reranked_merged.iloc[0], ABSTRACTS_OR_SNIPPETS)\n",
    "        )\n",
    "    else:\n",
    "        exact_answer = return_exact_answer(\n",
    "            reranked_merged.iloc[0], ABSTRACTS_OR_SNIPPETS\n",
    "        )\n",
    "    json_list.append(\n",
    "        {\n",
    "            \"type\": reranked_merged[\"questiontype\"].tolist()[0],\n",
    "            \"body\": reranked_merged[\"question\"].tolist()[0],\n",
    "            \"id\": reranked_merged[\"questionno\"].tolist()[0],\n",
    "            \"ideal_answer\": reranked_merged[\n",
    "                f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"\n",
    "            ].tolist()[0][0],\n",
    "            \"exact_answer\": exact_answer,\n",
    "            \"documents\": reranked_merged[\"url\"].tolist(),\n",
    "            \"snippets\": return_snippets(reranked_merged, choice)[:10],  # not good\n",
    "        }\n",
    "    )\n",
    "\n",
    "    json_out = {\"questions\": json_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/submission/system_1.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers based on abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR1 = f\"temp/answers/openai/{choice}\"\n",
    "IN_DIR2 = \"temp/snippets/gpt\"\n",
    "IN_DIR3 = \"temp/snippets/mistral\"\n",
    "\n",
    "ABSTRACTS_OR_SNIPPETS = \"abstracts\"\n",
    "\n",
    "json_list = []\n",
    "\n",
    "for file in tqdm(os.listdir(IN_DIR1)):\n",
    "    reranked1 = pd.read_csv(os.path.join(IN_DIR1, file))\n",
    "    # reranked2 = pd.read_csv(os.path.join(IN_DIR2, file))\n",
    "    reranked3 = pd.read_csv(os.path.join(IN_DIR3, file))\n",
    "    # reranked_merged = pd.concat([reranked1, reranked2, reranked3], axis=1)\n",
    "    reranked_merged = pd.concat([reranked1, reranked3], axis=1)\n",
    "    reranked_merged = reranked_merged.loc[:, ~reranked_merged.columns.duplicated()]\n",
    "    reranked_merged[f\"title_snippets_{choice}\"] = reranked_merged[\n",
    "        f\"title_snippets_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"abstract_snippets_{choice}\"] = reranked_merged[\n",
    "        f\"abstract_snippets_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"offset_title_{choice}\"] = reranked_merged[\n",
    "        f\"offset_title_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"offset_abstract_{choice}\"] = reranked_merged[\n",
    "        f\"offset_abstract_{choice}\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"] = reranked_merged[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"\n",
    "    ].apply(ast.literal_eval)\n",
    "    reranked_merged[f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"] = reranked_merged[\n",
    "        f\"answer_{ABSTRACTS_OR_SNIPPETS}_exact\"\n",
    "    ].apply(ast.literal_eval)\n",
    "\n",
    "    if reranked_merged[\"questiontype\"].tolist()[0] in (\"factoid\", \"list\"):\n",
    "        exact_answer = fix_list(\n",
    "            return_exact_answer(reranked_merged.iloc[0], ABSTRACTS_OR_SNIPPETS)\n",
    "        )\n",
    "    else:\n",
    "        exact_answer = return_exact_answer(\n",
    "            reranked_merged.iloc[0], ABSTRACTS_OR_SNIPPETS\n",
    "        )\n",
    "\n",
    "    json_list.append(\n",
    "        {\n",
    "            \"type\": reranked_merged[\"questiontype\"].tolist()[0],\n",
    "            \"body\": reranked_merged[\"question\"].tolist()[0],\n",
    "            \"id\": reranked_merged[\"questionno\"].tolist()[0],\n",
    "            \"ideal_answer\": reranked_merged[\n",
    "                f\"answer_{ABSTRACTS_OR_SNIPPETS}_ideal\"\n",
    "            ].tolist()[0][0],\n",
    "            \"exact_answer\": exact_answer,\n",
    "            \"documents\": reranked_merged[\"url\"].tolist(),\n",
    "            \"snippets\": return_snippets(reranked_merged, choice)[:10],  # not good\n",
    "        }\n",
    "    )\n",
    "\n",
    "    json_out = {\"questions\": json_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/submission/system_2.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['documents', 'snippets', 'id', 'type', 'body'])\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "TEST_FILE = \"temp/batch_4/test_set/BioASQ-task12bPhaseB-testset4\"\n",
    "\n",
    "with open(TEST_FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[\"questions\"]\n",
    "print(data[0].keys())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets_from_df(row):\n",
    "    return [x[\"text\"] for x in row.snippets]\n",
    "\n",
    "\n",
    "def fix_list(l):\n",
    "    l_out = []\n",
    "    for item in l:\n",
    "        if type(item) == list:\n",
    "            if len(item) == 1 and [item] not in l_out:\n",
    "                l_out.append(item)\n",
    "            elif len(item) > 1:\n",
    "                for subitem in item:\n",
    "                    if [subitem] not in l_out:\n",
    "                        l_out.append([subitem])\n",
    "        if type(item) == str and [item] not in l_out:\n",
    "            l_out.append([item])\n",
    "\n",
    "    return l_out[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers based on snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [09:37<00:00,  6.79s/it]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(data):\n",
    "    df = pd.json_normalize(d)\n",
    "    df[\"snippets_extracted\"] = df.apply(get_snippets_from_df, axis=1)\n",
    "    df.rename(\n",
    "        columns={\"body\": \"question\", \"id\": \"questionno\", \"type\": \"questiontype\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    df[\"answer_snippets_exact\"] = [\n",
    "        response_exact_answer(\n",
    "            df[\"question\"].values[0],\n",
    "            df[\"questiontype\"].values[0],\n",
    "            \" \".join(df.snippets_extracted.tolist()[0]),\n",
    "        )\n",
    "    ] * len(df)\n",
    "    df[\"answer_snippets_ideal\"] = [\n",
    "        response_ideal_answer(\n",
    "            df[\"question\"].values[0],\n",
    "            df[\"questiontype\"].values[0],\n",
    "            \" \".join(df.snippets_extracted.tolist()[0]),\n",
    "        )\n",
    "    ] * len(df)\n",
    "\n",
    "    df.to_csv(\n",
    "        f\"temp/batch_4/phase_b/answers/openai/snippets/{df.questionno.values[0]}.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "\n",
    "IN_DIR = \"temp/batch_4/phase_b/answers/openai/snippets/\"\n",
    "\n",
    "for file in os.listdir(IN_DIR):\n",
    "    df = pd.read_csv(os.path.join(IN_DIR, file))\n",
    "    df[\"answer_snippets_exact\"] = df[\"answer_snippets_exact\"].apply(ast.literal_eval)\n",
    "    df[\"answer_snippets_ideal\"] = df[\"answer_snippets_ideal\"].apply(ast.literal_eval)\n",
    "\n",
    "    if df[\"questiontype\"].values[0] == \"summary\":\n",
    "        json_list.append(\n",
    "            {\n",
    "                \"type\": df[\"questiontype\"].values[0],\n",
    "                \"body\": df[\"question\"].values[0],\n",
    "                \"id\": df[\"questionno\"].values[0],\n",
    "                \"ideal_answer\": df[\"answer_snippets_ideal\"].values[0][0],\n",
    "                \"documents\": df[\"documents\"].apply(ast.literal_eval).tolist()[0],\n",
    "                \"snippets\": df[\"snippets\"].apply(ast.literal_eval).tolist()[0],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        if df[\"questiontype\"].values[0] in (\"factoid\", \"list\"):\n",
    "            exact_answer = fix_list(df[\"answer_snippets_exact\"].values[0])\n",
    "        else:\n",
    "            exact_answer = df[\"answer_snippets_exact\"].values[0][0]\n",
    "\n",
    "        json_list.append(\n",
    "            {\n",
    "                \"type\": df[\"questiontype\"].values[0],\n",
    "                \"body\": df[\"question\"].values[0],\n",
    "                \"id\": df[\"questionno\"].values[0],\n",
    "                \"ideal_answer\": df[\"answer_snippets_ideal\"].values[0][0],\n",
    "                \"exact_answer\": exact_answer,\n",
    "                \"documents\": df[\"documents\"].apply(ast.literal_eval).tolist()[0],\n",
    "                \"snippets\": df[\"snippets\"].apply(ast.literal_eval).tolist()[0],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    json_out = {\"questions\": json_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/batch_4/phase_b/submission/mibi_rag_snippet.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in json_list:\n",
    "#     try:\n",
    "#         print(d[\"exact_answer\"])\n",
    "#     except:\n",
    "#         print(\"NO ANSWER\")\n",
    "#     print(d[\"ideal_answer\"])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers based on abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANK_CROSS = 10\n",
    "RERANK_BI = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_pubmed_documents import get_title_abstract\n",
    "\n",
    "ABSTRACTS_FOR_ANSWER = 3\n",
    "\n",
    "for d in tqdm(data):\n",
    "    df = pd.json_normalize(d)\n",
    "    df[\"snippets_extracted\"] = df.apply(get_snippets_from_df, axis=1)\n",
    "    df.rename(\n",
    "        columns={\"body\": \"question\", \"id\": \"questionno\", \"type\": \"questiontype\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    titles_abstracts = [get_title_abstract(url) for url in df[\"documents\"].tolist()[0]]\n",
    "    titles = list(zip(*titles_abstracts))[0]\n",
    "    abstracts = list(zip(*titles_abstracts))[1]\n",
    "    reranked = pd.DataFrame({\"title\": titles, \"abstract\": abstracts})\n",
    "    question = Question(\n",
    "        id=df[\"questionno\"].values[0],\n",
    "        type=df[\"questiontype\"].values[0],\n",
    "        body=df[\"question\"].values[0],\n",
    "    )\n",
    "\n",
    "    reranked = pd.DataFrame({\"title\": titles, \"text\": abstracts})\n",
    "    reranked[\"cos_sim\"] = rerank_crossencoder(question, reranked)\n",
    "    reranked = reranked.sort_values(\"cos_sim\", ascending=False).head(RERANK_CROSS)\n",
    "    reranked = reranked.drop(\"cos_sim\", axis=1)\n",
    "\n",
    "    reranked[\"cos_sim\"] = rerank_biencoder(question, reranked)\n",
    "    reranked = reranked.sort_values(\"cos_sim\", ascending=False).head(RERANK_BI)\n",
    "\n",
    "    df[\"answer_abstracts_exact\"] = [\n",
    "        response_exact_answer(\n",
    "            df[\"question\"].values[0],\n",
    "            df[\"questiontype\"].values[0],\n",
    "            \" \".join(reranked[\"text\"].tolist()[:ABSTRACTS_FOR_ANSWER]),\n",
    "        )\n",
    "    ] * len(df)\n",
    "    df[\"answer_abstracts_ideal\"] = [\n",
    "        response_ideal_answer(\n",
    "            df[\"question\"].values[0],\n",
    "            df[\"questiontype\"].values[0],\n",
    "            \" \".join(reranked[\"text\"].tolist()[:ABSTRACTS_FOR_ANSWER]),\n",
    "        )\n",
    "    ] * len(df)\n",
    "\n",
    "    df.to_csv(\n",
    "        f\"temp/batch_4/phase_b/answers/openai/abstracts/{df.questionno.values[0]}.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "\n",
    "IN_DIR = \"temp/batch_4/phase_b/answers/openai/abstracts/\"\n",
    "\n",
    "for file in os.listdir(IN_DIR):\n",
    "    df = pd.read_csv(os.path.join(IN_DIR, file))\n",
    "    df[\"answer_abstracts_exact\"] = df[\"answer_abstracts_exact\"].apply(ast.literal_eval)\n",
    "    df[\"answer_abstracts_ideal\"] = df[\"answer_abstracts_ideal\"].apply(ast.literal_eval)\n",
    "\n",
    "    if df[\"questiontype\"].values[0] == \"summary\":\n",
    "        json_list.append(\n",
    "            {\n",
    "                \"type\": df[\"questiontype\"].values[0],\n",
    "                \"body\": df[\"question\"].values[0],\n",
    "                \"id\": df[\"questionno\"].values[0],\n",
    "                \"ideal_answer\": df[\"answer_abstracts_ideal\"].values[0][0],\n",
    "                \"documents\": df[\"documents\"].apply(ast.literal_eval).tolist()[0],\n",
    "                \"snippets\": df[\"snippets\"].apply(ast.literal_eval).tolist()[0],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        if df[\"questiontype\"].values[0] in (\"factoid\", \"list\"):\n",
    "            exact_answer = fix_list(df[\"answer_abstracts_exact\"].values[0])\n",
    "        else:\n",
    "            exact_answer = df[\"answer_abstracts_exact\"].values[0][0]\n",
    "\n",
    "        json_list.append(\n",
    "            {\n",
    "                \"type\": df[\"questiontype\"].values[0],\n",
    "                \"body\": df[\"question\"].values[0],\n",
    "                \"id\": df[\"questionno\"].values[0],\n",
    "                \"ideal_answer\": df[\"answer_abstracts_ideal\"].values[0][0],\n",
    "                \"exact_answer\": exact_answer,\n",
    "                \"documents\": df[\"documents\"].apply(ast.literal_eval).tolist()[0],\n",
    "                \"snippets\": df[\"snippets\"].apply(ast.literal_eval).tolist()[0],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    json_out = {\"questions\": json_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/batch_4/phase_b/submission/mibi_rag_abstract.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in json_list:\n",
    "#     print(d[\"body\"])\n",
    "#     try:\n",
    "#         print(d[\"exact_answer\"])\n",
    "#     except:\n",
    "#         print(\"NO ANSWER\")\n",
    "#     print(d[\"ideal_answer\"])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranked[\"snippets_blablador_gpt\"] = reranked.apply(\n",
    "#     lambda x: get_snippets_blablador(\n",
    "#         question.body, x[\"title\"], x[\"text\"], model=\"gpt-3.5-turbo\"\n",
    "#     ),\n",
    "#     axis=1,\n",
    "# )\n",
    "# reranked[\"snippets_blablador_mistral\"] = reranked.apply(\n",
    "#     lambda x: get_snippets_blablador(\n",
    "#         question.body, x[\"title\"], x[\"text\"], model=\"Mistral-7B-Instruct-v0.2\"\n",
    "#     ),\n",
    "#     axis=1,\n",
    "# )\n",
    "# reranked[\"title_snippets_gpt\"] = [x[0] for x in reranked[\"snippets_blablador_gpt\"]]\n",
    "# reranked[\"abstract_snippets_gpt\"] = [\n",
    "#     x[1] for x in reranked[\"snippets_blablador_gpt\"]\n",
    "# ]\n",
    "# reranked[\"title_snippets_mistral\"] = [\n",
    "#     x[0] for x in reranked[\"snippets_blablador_mistral\"]\n",
    "# ]\n",
    "# reranked[\"abstract_snippets_mistral\"] = [\n",
    "#     x[1] for x in reranked[\"snippets_blablador_mistral\"]\n",
    "# ]\n",
    "\n",
    "# reranked[\"offset_title_gpt\"] = reranked.apply(\n",
    "#     lambda x: get_offset(x[\"title_snippets_gpt\"], x[\"text\"]), axis=1\n",
    "# )\n",
    "# reranked[\"offset_abstract_gpt\"] = reranked.apply(\n",
    "#     lambda x: get_offset(x[\"abstract_snippets_gpt\"], x[\"text\"]), axis=1\n",
    "# )\n",
    "\n",
    "# reranked[\"offset_title_mistral\"] = reranked.apply(\n",
    "#     lambda x: get_offset(x[\"title_snippets_mistral\"], x[\"text\"]), axis=1\n",
    "# )\n",
    "# reranked[\"offset_abstract_mistral\"] = reranked.apply(\n",
    "#     lambda x: get_offset(x[\"abstract_snippets_mistral\"], x[\"text\"]), axis=1\n",
    "# )\n",
    "\n",
    "\n",
    "# reranked[\"answer_abstracts_exact\"] = [\n",
    "#     response_exact_answer(\n",
    "#         question.body,\n",
    "#         question.type,\n",
    "#         \" \".join(reranked[\"text\"].tolist()[:ABSTRACTS_FOR_ANSWER]),\n",
    "#     )\n",
    "# ] * len(reranked)\n",
    "# reranked[\"answer_abstracts_ideal\"] = [\n",
    "#     response_ideal_answer(\n",
    "#         question.body,\n",
    "#         question.type,\n",
    "#         \" \".join(reranked[\"text\"].tolist()[:ABSTRACTS_FOR_ANSWER]),\n",
    "#     )\n",
    "# ] * len(reranked)\n",
    "# reranked[\"answer_snippets_exact\"] = [\n",
    "#     response_exact_answer(\n",
    "#         question.body,\n",
    "#         question.type,\n",
    "#         \" \".join(\n",
    "#             flat_list(reranked[\"abstract_snippets\"].tolist())\n",
    "#             + flat_list(reranked[\"title_snippets\"])\n",
    "#         ),\n",
    "#     )\n",
    "# ] * len(reranked)\n",
    "# reranked[\"answer_snippets_ideal\"] = [\n",
    "#     response_ideal_answer(\n",
    "#         question.body,\n",
    "#         question.type,\n",
    "#         \" \".join(\n",
    "#             flat_list(reranked[\"abstract_snippets\"].tolist())\n",
    "#             + flat_list(reranked[\"title_snippets\"])\n",
    "#         ),\n",
    "#     )\n",
    "# ] * len(reranked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
